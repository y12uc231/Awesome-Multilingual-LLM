# Awesome-Multilingual-LLM

The repository is designed to support the growing interest within the community in developing large language models (LLMs) that cater not only to English speakers but also to speakers of the other 6,500+ languages worldwide. Its purpose is to aid researchers in discovering pertinent literature in this field. The repository will encompass a comprehensive collection of core training and evaluation datasets, multilingual-capable LLMs, and associated scholarly articles.

### 1. **In-Context Learning and Prompting Strategies**
- [Tanwar et al.](https://dx.doi.org/10.48550/arXiv.2305.05940): Discusses a novel prompt construction strategy, Cross-lingual In-context Source Target Alignment (X-InSTA), for in-context learning in multilingual settings.
- [Liu et al.](https://dx.doi.org/10.48550/arXiv.2309.08591): Investigates the ability of multilingual LLMs in understanding proverbs and sayings in conversational contexts.

### 2. **Performance and Capabilities in Specific Languages**
- [Holmström et al.](https://www.aclweb.org/anthology/2023.resourceful-1.13): Explores the performance of English and multilingual LLMs in Swedish.

### 3. **Challenges and Limitations in Multilingual LLMs**
- [Zhu et al.](https://dx.doi.org/10.48550/arXiv.2304.04675): Investigates the advantages and challenges in multilingual machine translation using LLMs.

### 4. **Multilingual LLMs in Programming and Code**
- [Joshi et al.](https://dx.doi.org/10.48550/arXiv.2208.11640): Introduces RING, a multilingual repair engine powered by a language model trained on code.

### 5. **Comparative Studies and Benchmarks**
- [Ahuja et al.](https://dx.doi.org/10.48550/arXiv.2211.05100): Discusses the development and evaluation of BLOOM, a 176B-parameter open-access multilingual language model.
- [Lai et al.](https://dx.doi.org/10.48550/arXiv.2304.05613): Evaluates ChatGPT and other LLMs on multilingual NLP tasks.

### 6. **Datasets And Benchmarks**

- **Multilingual Multimodal Learning with Machine Translated Text** by Chen Qiu et al. investigates using machine-translated English multimodal data for training multilingual LLMs. They propose a framework called TD-MML and demonstrate its effectiveness in the IGLUE benchmark across 20 languages ([Qiu et al.](https://dx.doi.org/10.48550/arXiv.2210.13134)).

- **The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset** by Hugo Laurenccon et al. presents the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages, used to train the BigScience Large Open-science Open-access Multilingual (BLOOM) language model ([Laurenccon et al.](https://dx.doi.org/10.48550/arXiv.2303.03915)).

- **XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond** by Francesco Barbieri et al. introduces a dataset for multilingual LLMs using tweets in over thirty languages, along with sentiment analysis Twitter datasets in eight different languages ([Barbieri et al.](https://arxiv.org/abs/2104.12250)).

- **Generating Extended and Multilingual Summaries with Pre-trained Transformers** by Rémi Calizzano et al. introduces the WikinewsSum dataset for multilingual summarization, including news articles in seven languages tailored for extended summaries ([Calizzano et al.](https://www.aclweb.org/anthology/2022.lrec-1.175)).

- A paper by Po-Yao (Bernie) Huang et al. proposes Multi-HowTo100M, a multilingual instructional video dataset for pre-training models to improve video search in non-English languages ([Huang et al.](https://dx.doi.org/10.18653/V1/2021.NAACL-MAIN.195)).

- **PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification** by Yinfei Yang et al. proposes PAWS-X, consisting of human-translated paraphrase identification pairs in six languages, to evaluate multilingual LLMs ([Yang et al.](https://dx.doi.org/10.18653/v1/D19-1382)).

- **African News Corpus** by David Ifeoluwa Adelani et al. covers 16 African languages, including 8 languages not part of any existing evaluation dataset, demonstrating the effectiveness of leveraging high-quality translation data ([Adelani et al.](https://dx.doi.org/10.48550/arXiv.2205.02022)).

- **REDFM: a Filtered and Multilingual Relation Extraction Dataset** by Pere-Lluís Huguet Cabot et al. introduces SREDFM and REDFM datasets for training and evaluating multilingual relation extraction systems ([Cabot et al.](https://dx.doi.org/10.48550/arXiv.2306.09802)).

-  **GINCO Training Dataset** by Taja Kuzman et al. consists of 1,125 Slovenian web documents for genre identification, showing that Transformer-based models perform well on this task ([Kuzman et al.](https://arxiv.org/abs/2201.03857)).
- [Li et al.](https://dx.doi.org/10.48550/arXiv.2306.04387): Introduces the M$^3$IT dataset for optimizing the alignment of vision-language models with human instructions.

### 7. **Translation and Language Understanding**
- [Guerreiro et al.](https://dx.doi.org/10.1162/tacl_a_00615): Provides insights into the presence of hallucinations in multilingual translation models.
- [Li et al.](https://dx.doi.org/10.48550/arXiv.2305.15083): Discusses the translation abilities of large language models in multilingual contexts.

### 8. **Miscellaneous Studies and Surveys**
- [Pahune et al.](https://dx.doi.org/10.22214/ijraset.2023.54677): Emphasizes recent developments and efforts made for various kinds of LLMs, including multilingual language models.
