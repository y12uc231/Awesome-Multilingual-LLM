# Awesome-Multilingual-LLM

The repository is designed to support the growing interest within the community in developing large language models (LLMs) that cater not only to English speakers but also to speakers of the other 6,500+ languages worldwide. Its purpose is to aid researchers in discovering pertinent literature in this field. The repository will encompass a comprehensive collection of core training and evaluation datasets, multilingual-capable LLMs, and associated scholarly articles.

### 1. **In-Context Learning and Prompting Strategies**
- [Tanwar et al.](https://dx.doi.org/10.48550/arXiv.2305.05940): Discusses a novel prompt construction strategy, Cross-lingual In-context Source Target Alignment (X-InSTA), for in-context learning in multilingual settings.
- [Liu et al.](https://dx.doi.org/10.48550/arXiv.2309.08591): Investigates the ability of multilingual LLMs in understanding proverbs and sayings in conversational contexts.

### 2. **Performance and Capabilities in Specific Languages**
- [Holmstr√∂m et al.](https://www.aclweb.org/anthology/2023.resourceful-1.13): Explores the performance of English and multilingual LLMs in Swedish.

### 3. **Challenges and Limitations in Multilingual LLMs**
- [Zhu et al.](https://dx.doi.org/10.48550/arXiv.2304.04675): Investigates the advantages and challenges in multilingual machine translation using LLMs.

### 4. **Multilingual LLMs in Programming and Code**
- [Joshi et al.](https://dx.doi.org/10.48550/arXiv.2208.11640): Introduces RING, a multilingual repair engine powered by a language model trained on code.

### 5. **Comparative Studies and Benchmarks**
- [Ahuja et al.](https://dx.doi.org/10.48550/arXiv.2211.05100): Discusses the development and evaluation of BLOOM, a 176B-parameter open-access multilingual language model.
- [Lai et al.](https://dx.doi.org/10.48550/arXiv.2304.05613): Evaluates ChatGPT and other LLMs on multilingual NLP tasks.

### 6. **Datasets and Benchmarks for Evaluation**
- [Li et al.](https://dx.doi.org/10.48550/arXiv.2306.04387): Introduces the M$^3$IT dataset for optimizing the alignment of vision-language models with human instructions.

### 7. **Translation and Language Understanding**
- [Guerreiro et al.](https://dx.doi.org/10.1162/tacl_a_00615): Provides insights into the presence of hallucinations in multilingual translation models.
- [Li et al.](https://dx.doi.org/10.48550/arXiv.2305.15083): Discusses the translation abilities of large language models in multilingual contexts.

### 8. **Miscellaneous Studies and Surveys**
- [Pahune et al.](https://dx.doi.org/10.22214/ijraset.2023.54677): Emphasizes recent developments and efforts made for various kinds of LLMs, including multilingual language models.
